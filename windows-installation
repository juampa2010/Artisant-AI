How-to process flow in a Windows LOCAL machine

Note that this process was tested on a Windows LOCAL machine which serves as the Server machine, and this process was done for fun and as an example to have something to be tested and running quickly.

1. Go to https://ollama.com/ and download it.
  Sign up with your email to get notified of new updates.
  Select Windows (Preview) and download it to your Windows machine.

2. Run the OllamaSetup.exe file on your Windows machine. Approx: 198MB size.
3. Follow the installation steps. Click Install to extract files and it will be installed in your machine.
4. To do a quick check to see if it was properly installed go to a CMD or Powershell and type:
   
  > ollama

  You should see something like this:
  Usage:
    ollama [flags]
    ollama [command]

  Available Commands:
    serve       Start ollama
    create      Create a model from a Modelfile
    show        Show information for a model
    run         Run a model
    pull        Pull a model from a registry
    push        Push a model to a registry
    list        List models
    ps          List running models
    cp          Copy a model
    rm          Remove a model
    help        Help about any command

  Flags:
    -h, --help      help for ollama
    -v, --version   Show version information

  Use "ollama [command] --help" for more information about a command.

5. Next step is to download the AI model or models that we are going to be using for this example.
   If you want to see the complete list of ollama.ai models go to: https://ollama.com/library

   We are going to have to Cooks, the expert one will be using latest llama3 ai model, the novice onw will be using a llama2 ai model.
   Diferences are in the number of parameters each of them are using. 
   In this example we want to give more expertise to the expert cook than to the novice cook.

   From the CMD / Powershell run these 2 below commands. Make sure you at least have 10GB disk free space to allocate both.
   The 'run' command will try to run the model locally where you can start testing it by prompting.
   If the model isn´t in your local installation it will be downloaded first (pulling manifest ...)
   It will take some time to download the models to your local installation.

   > ollama run llama3 
   Sample prompt: >>> Hi, why water is transparent?
   <You should see the answer>
   >>> /bye

   > ollama run llama2
   Sample prompt: >>> Hi, why makes rainbow show those colors?
   <You should see the answer>
   >>> /bye

6. Use one of your python coding tools and create a new python proyect and call it 'Chatbot_Cooking' for example.
7. Within the project create 2 different Modelfiles that will be used by the ai models to have domain specific answers, in this case related to Cooking.

   Modelfiles are used by the ai models to parametrize your ai models and make them more domain specific. For example, my expert model file is going to be more precise since it will be using llama3, and it will also contain a description of what that ai model is all about.
   
   See examples, notice that this Chatbot will answer in Spanish:
     
   Model file 'Modelfile_expert' for the expert cook:

      FROM llama3
      # set the temperature to 1 [higher is more creative, lower is more coherent]
      # PARAMETER temperature 0.75 
      PARAMETER temperature 1
      
      # Tamaño de la ventana de contexto para controlar el número de tokens que el modelo LLM puede usar como contexto para generar el siguiente token
      # En otras palabras, determina cuánto "recuerda" el modelo al generar texto.
      # El valor por defecto, lo he subido al doble
      PARAMETER num_ctx 4096
      
      # set the system prompt
      SYSTEM """
      Eres un sistema experto de recetas y nutrición.
      Ayuda a personas de todo tipo y condición a crear recetas muy nutritivas y saludables.
      Te llamas Alex y eres el gran experto evangelista de la cocina moderna y vanguardista.
      Siempre vas a contestar en español castellano.
      """
   
   Model file 'Modelfile_aprendiz' for the novice cook:

      FROM llama2
      # set the temperature to 1 [higher is more creative, lower is more coherent]
      # PARAMETER temperature 0.75 
      PARAMETER temperature 0.75
      
      # Tamaño de la ventana de contexto para controlar el número de tokens que el modelo LLM puede usar como contexto para generar el siguiente token
      # En otras palabras, determina cuánto "recuerda" el modelo al generar texto.
      # El valor por defecto, lo he subido al doble
      PARAMETER num_ctx 4096
      
      # set the system prompt
      SYSTEM """
      Eres un sistema aprendiz de recetas y nutrición. 
      Ayuda a personas de todo tipo y condición a crear recetas muy nutritivas y saludables. 
      Te llamas Juan y eres el gran aprendiz de la cocina moderna y vanguardista. Siempre vas a contestar en español o castellano.
      """

7. Now run from your CMD/Powershell console the following commands.

   Expert cook - a reference to the llama3 model will be created using the expert model file
   > ollama create alex_experto -f ./Modelfile_experto

   Novice cook - a reference to the llama2 model will be created using the expert model file
   > ollama create juan_aprendiz -f ./Modelfile_aprendiz

8. If no issues you can list all the models using command > ollama list

   You should see something like this:
    NAME                    ID              SIZE    MODIFIED
    juan_aprendiz:latest    643daf25f2e1    3.8 GB  2 days ago
    alex_experto:latest     3e08413ffd88    4.7 GB  2 days ago
    llama2:latest           78e26419b446    3.8 GB  8 days ago
    llama3:latest           a6990ed6be41    4.7 GB  2 weeks ago

9. 
