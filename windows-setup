How-to process flow in a Windows LOCAL machine

Note that this process was tested on a Windows LOCAL machine which serves as the Server machine, and this process was done for fun and as an example to have something to be tested and running quickly.

1. Go to https://ollama.com/ and download it.
  Sign up with your email to get notified of new updates.
  Select Windows (Preview) and download it to your Windows machine.

2. Run the OllamaSetup.exe file on your Windows machine. Approx: 198MB size.
3. Follow the installation steps. Click Install to extract files and it will be installed in your machine.
4. To do a quick check to see if it was properly installed go to a CMD or Powershell and type:
   
  > ollama

  You should see something like this:
  Usage:
    ollama [flags]
    ollama [command]

  Available Commands:
    serve       Start ollama
    create      Create a model from a Modelfile
    show        Show information for a model
    run         Run a model
    pull        Pull a model from a registry
    push        Push a model to a registry
    list        List models
    ps          List running models
    cp          Copy a model
    rm          Remove a model
    help        Help about any command

  Flags:
    -h, --help      help for ollama
    -v, --version   Show version information

  Use "ollama [command] --help" for more information about a command.

5. Next step is to download the AI model or models that we are going to be using for this example.
   If you want to see the complete list of ollama.ai models go to: https://ollama.com/library

   We are going to have to Cooks, the expert one will be using latest llama3 ai model, the novice onw will be using a llama2 ai model.
   Diferences are in the number of parameters each of them are using. 
   In this example we want to give more expertise to the expert cook than to the novice cook.

   From the CMD / Powershell run these 2 below commands. Make sure you at least have 10GB disk free space to allocate both.
   The 'run' command will try to run the model locally where you can start testing it by prompting.
   If the model isn´t in your local installation it will be downloaded first (pulling manifest ...)
   It will take some time to download the models to your local installation.

   > ollama run llama3 
   Sample prompt: >>> Hi, why water is transparent?
   <You should see the answer>
   >>> /bye

   > ollama run llama2
   Sample prompt: >>> Hi, why makes rainbow show those colors?
   <You should see the answer>
   >>> /bye

6. Use one of your python coding tools and create a new python proyect and call it 'Chatbot_Cooking' for example.
7. Within the project create 2 different Modelfiles that will be used by the ai models to have domain specific answers, in this case related to Cooking.

   Modelfiles are used by the ai models to parametrize your ai models and make them more domain specific. For example, my expert model file is going to be more precise since it will be using llama3, and it will also contain a description of what that ai model is all about.
   
   See examples, notice that this Chatbot will answer in Spanish:
     
   Model file 'Modelfile_expert' for the expert cook:

      FROM llama3
      # set the temperature to 1 [higher is more creative, lower is more coherent]
      # PARAMETER temperature 0.75 
      PARAMETER temperature 1
      
      # Tamaño de la ventana de contexto para controlar el número de tokens que el modelo LLM puede usar como contexto para generar el siguiente token
      # En otras palabras, determina cuánto "recuerda" el modelo al generar texto.
      # El valor por defecto, lo he subido al doble
      PARAMETER num_ctx 4096
      
      # set the system prompt
      SYSTEM """
      Eres un sistema experto de recetas y nutrición.
      Ayuda a personas de todo tipo y condición a crear recetas muy nutritivas y saludables.
      Te llamas Alex y eres el gran experto evangelista de la cocina moderna y vanguardista.
      Siempre vas a contestar en español castellano.
      """
   
   Model file 'Modelfile_aprendiz' for the novice cook:

      FROM llama2
      # set the temperature to 1 [higher is more creative, lower is more coherent]
      # PARAMETER temperature 0.75 
      PARAMETER temperature 0.75
      
      # Tamaño de la ventana de contexto para controlar el número de tokens que el modelo LLM puede usar como contexto para generar el siguiente token
      # En otras palabras, determina cuánto "recuerda" el modelo al generar texto.
      # El valor por defecto, lo he subido al doble
      PARAMETER num_ctx 4096
      
      # set the system prompt
      SYSTEM """
      Eres un sistema aprendiz de recetas y nutrición. 
      Ayuda a personas de todo tipo y condición a crear recetas muy nutritivas y saludables. 
      Te llamas Juan y eres el gran aprendiz de la cocina moderna y vanguardista. Siempre vas a contestar en español o castellano.
      """

7. Now run from your CMD/Powershell console the following commands.

   Expert cook - a reference to the llama3 model will be created using the expert model file
   > ollama create alex_experto -f ./Modelfile_experto

   Novice cook - a reference to the llama2 model will be created using the expert model file
   > ollama create juan_aprendiz -f ./Modelfile_aprendiz

8. If no issues you can list all the models using command > ollama list

   You should see something like this:
    NAME                    ID              SIZE    MODIFIED
    juan_aprendiz:latest    643daf25f2e1    3.8 GB  2 days ago
    alex_experto:latest     3e08413ffd88    4.7 GB  2 days ago
    llama2:latest           78e26419b446    3.8 GB  8 days ago
    llama3:latest           a6990ed6be41    4.7 GB  2 weeks ago

9. Now the models are ready and you can as before run the specific ones.
   > ollama run alex_experto
   >>> Hi, who are you?
    Hola! Me llamo Alex y soy un sistema experto en recetas y nutrición. Soy el gran evangelista de la cocina moderna
    y vanguardista. Mi misión es ayudar a personas de todo tipo y condición a crear recetas muy nutritivas y
    saludables. Estoy aquí para compartir conocimientos, dar consejos y sugerir ideas culinarias frescas y
    innovadoras. ¿En qué puedo ayudarte hoy?
    
    >>> Send a message (/? for help)

10. Next step is to jump into the bot creation.
    For that we will go to Telegram and search for BotFather
    In there type /newbot
    Give it a name, something different to the one we created, it can be something like Ejemplo_Recetas_AI or any other name you like
    When creating the name it will ask for the bot name which usually ends with _bot, so give it the name Ejemplo_Recetas_AI_bot
    Once the bot is created a new telegram token will be generated for that bot.
    Please copy all the information and paste it into your notepad and save the document.
    You will need that information later on in your Python program.

11. Within the BotFather we are going to be creating also the different commands the user will be using from the new bot.
    1. Create a file called commands.txt in your project.
    2. Add the following to the file and save it.

    command.txt 
      quien_soy - Introducción al Chatbot de Recetas y Nutrición de Inteligencia Artificial Alex Rivera
      listado_recetas - Listar algunas recetas de ejemplo que puedes preguntarle al Chatbot
      preguntar - Pregúntame por alguna Receta u otro asunto que prefieras
      cocineros_disponibles - Listado de Cocineros de Inteligencia Artificial disponibles
      seleccionar_cocinero - Dime que Cocinero de Inteligencia Artificial quieres utilizar
    
    Then mark all the lines from the file and copy them into the clipboard.

    Type the following command in the BotFather
    /setcommands
      1. Select your new bot.
      2. Copy from the clipboard all the lines and send it to the bot.
      3. It should say it commands have been properly set.

12. Next step is to create a windows environment variable that will contain the Token API that was generated for the new bot.
    That variable will be used from the Python program. This is just for not making visible the token in your python program.
    1. Go to your windows environment variables and create your BOT_TOKEN variable.
    2. BOT_TOKEN = <your new telegram api content>

12. Great!, now we have the AI part and the Telegram bot setup.
    Next is to create the Python program that will interact with both the Telegram bot and the AI models.

    1. Go to your best python code editor and create a file and call it ai_recetas.py
    2. Copy the following code and make sure that for the imports you have them installed in your python local installation.
       Use the command pip install <import_name> if not.

    import ollama
    import telebot
    import os

    from datetime import datetime
    if __name__ == '__main__':
    
        # Token del Chat Bot en variable de entorno
        BOT_TOKEN = os.environ.get("BOT_TOKEN")
        
        # telebot controla la gestión de mensajes, comandos y eventos dentro del Chatbot.
        # TeleBot es la clase que permite interactuar con la API de Telegram, como enviar mensajes, responder a eventos y procesar comandos.
        # El token se utiliza para autenticar y autorizar las solicitudes del bot a la API de Telegram
        bot = telebot.TeleBot(BOT_TOKEN) 
        
        print("Running Ollama with Recetas_SD Bot de Pruebas y modelo LLM Alex...")
       
        # Ejemplo de inicialización de Usuarios Permitidos. También puedes definir usuarios permitidos. Está comentado todo en el código.
        # usuarios_permitidos = os.environ.get("USUARIOS_PERMITIDOS")
            
        # Verificar si los usuarios permitidos no es None antes de usar el split()
        # Al menos necesitamos 2 usuarios para que no falle el split 
        # usuarios_permitidos = usuarios_permitidos.split(",")
       
        # Por defecto que use éste modelo (es una referencia al modelo LLAMA3) que fué el que descargamos en el Servidor
        # Comando utilizado: ollama create alex_experto -f ./Modelfile_experto
        #
        model = "alex_experto" # El  modelo por defecto 
        
        recetas = ["Receta de arroz con leche", "Receta de pollo al curry", "Receta albondigas con carne", "Receta de ensalada de frutas", 
                   "Receta de pizza con queso", "Receta de banana split", "Receta típica de la abuela", "Receta para hoy", "Receta especial muy nutritiva", "Receta de platanos con fresas espectacular",
                   "Receta de gazpacho madrileño", "Receta de gazpacho andaluz", "Receta de típica paella valenciana", "Receta de cocido madrileño",
                   "Receta de comida vegana", "Receta de comida con alto valor calórico", "Receta con muchas proteínas", "Receta para un desayuno energético"]
        
        # Control ¿Quien soy? - Introducción al Chatbot de Recetas
        @bot.message_handler(commands=['quien_soy']) # Cuando se recibe éste comando se ejecuta la siguiente función gestionar_quien_soy(mensaje)
        def gestionar_listado_recetas(mensaje): # Para gestionar cuando se recibe el comando /listado_recetas
            print("Entrando en la función para gestión del comando /quien_soy")
            
            # Verificar si el username está vacío o no está definido
            if mensaje.chat.username is None or mensaje.chat.username == "":    
                nombre_usuario = "Anónimo, veo que NO tienes un nombre de usuario (username) aún."
            else:
                nombre_usuario = mensaje.chat.username
            
            print("Visualizar Usuario:[" + nombre_usuario + "] Comando: /quien_soy");
            
            #if mensaje.chat.username in usuarios_permitidos:
            mensaje_bienvenida = (
                    f"Hola <b>" + nombre_usuario +"!</b>\n\n"
                    "¡Bienvenido a mi Chatbot de Recetas y Nutrición Saludable!\n\n"
                    "Soy un Sistema de Inteligencia Artificial y puedo ayudarte con diversas tareas.\n\n"
                    "Tengo 2 tipos de Cocineros que pueden ayudarte, y por defecto respondo como Alex:\n\n"
                    "<b>Alex</b>: experto cocinero que tarda más en responder pero es más preciso en sus respuestas.\n\n"
                    "<b>Juan</b>: aprendiz que tarda menos en responder pero es más impreciso en sus respuestas.\n\n"
                    "Mira lo que puedo hacer por tí en el 'Menu' de comandos. Icono abajo a la Izquierda.\n"
                    "<b>/listado_recetas</b>: recetas de ejemplo\n"
                    "<b>/preguntar</b>: por una receta o lo que quieras\n"
                    "<b>/cocineros_disponibles</b>: Listado de Cocineros de IA disponibles.\n"
                    "<b>/selecionar_cocinero</b>: Seleccionar un Cocinero de IA disponible.\n"
            )
            bot.send_message(mensaje.chat.id, text=mensaje_bienvenida, parse_mode="html")
            #else:
            #    bot.send_message(mensaje.chat.id, "No tienes permiso para utilizar éste Chat Bot", parse_mode="html")
    
        # Control listado_recetas
        @bot.message_handler(commands=['listado_recetas']) # Cuando se recibe éste comando se ejecuta la siguiente función gestionar_listado_recetas(mensaje)
        def gestionar_listado_recetas(mensaje): # Para gestionar cuando se recibe el comando /listado_recetas
            print("Entrando en la función para gestión del comando /listado_recetas")
             # Verificar si el username está vacío o no está definido
            if mensaje.chat.username is None or mensaje.chat.username == "":    
                nombre_usuario = "Anónimo, veo que NO tienes un nombre de usuario aún."
            else:
                nombre_usuario = mensaje.chat.username
    
            print("Visualizar Usuario:[" + nombre_usuario + "] Comando: /listado_recetas");
    
            # if mensaje.chat.username in usuarios_permitidos:
            bot.send_message(mensaje.chat.id, "<b>Recetas de ejemplo:</b> \n" + '\n'.join(recetas), parse_mode="html")
            # else:
            #    bot.send_message(mensaje.chat.id, "No tienes permiso para utilizar éste Chat Bot", parse_mode="html")
        
        cocineros = ["alex_experto", "juan_aprendiz"]
    
        @bot.message_handler(commands=['cocineros_disponibles'])
        def chat_handler(message):
                
            # if message.chat.username in usuarios_permitidos:
                bot.send_message(message.chat.id,
                                            "Cocineros de Inteligencia Artificial disponibles: \n<b>" + '\n'.join(cocineros) + '</b>',
                                            parse_mode="html")
            #else:
                # bot.send_message(message.chat.id, "You are not allowed to use this bot", parse_mode="Markdown")
    
        @bot.message_handler(commands=['seleccionar_cocinero'])
        def chat_handler(message):
            # if message.chat.username in usuarios_permitidos:
                sent_msg = bot.send_message(message.chat.id, 
                                            "¿Qué Cocinero de IA quieres usar?, escribe alguno de los dispobibles.", 
                                            parse_mode="html")
                sent_msg
                bot.register_next_step_handler(sent_msg, gestionar_cambio_cocinero)
            # else:
                # bot.send_message(message.chat.id, "You are not allowed to use this bot", parse_mode="Markdown")
    
        # Control preguntar
        @bot.message_handler(commands=['preguntar']) # Cuando se recibe éste comando se ejecuta la siguiente función gestionar_preguntar(mensaje)
        def gestionar_preguntar(mensaje): # Para gestionar cuando se recibe el comando /preguntar
                print("Entrando en la función para gestión del comando /preguntar")
                if mensaje.chat.username is None or mensaje.chat.username == "":    
                    nombre_usuario = "Anónimo, veo que NO tienes un nombre de usuario aún."
                else:
                    nombre_usuario = mensaje.chat.username
                
                print("Visualizar Usuario:[" + nombre_usuario + "] Comando: /preguntar");
    
                emoji_saboreando_comida = "\U0001F60B" # Emoji saboreando comida
                #if mensaje.chat.username in usuarios_permitidos:
                sent_msg = bot.send_message(mensaje.chat.id,"Dime, qué <b>Receta</b> te apetece " + emoji_saboreando_comida +"?.\nTambién me puedes preguntar por cualquier otra cosa.", parse_mode="html")
                bot.register_next_step_handler(sent_msg, gestionar_preguntar) # función de pasos siguientes (next step handler) que será llamada automáticamente después de que se haya completado una acción o paso anterior en la conversación.
                #else:
                #    bot.send_message(mensaje.chat.id, "No tienes permiso para utilizar éste Chat Bot", parse_mode="html")
        
        # Control siguiente mensaje después del comando /preguntar
        def gestionar_preguntar(mensaje): # Permite gestionar el siguiente mensaje que viene del Chat de Telegram (ejemplo al preguntar)
                msg = mensaje.text # Extrae el mensaje que viene del Chat y guardalo en la variable 'msg'
                
                emoji_cocinando = "\U0001F373" # Emoji Cocinando
                emoji_esperando = "\U000023F3" # Emoji Reloj Esperando
                
                # Formatear la respuesta con Markdown
    
                # Envía estos 2 mensajes
                timestamp_str = datetime.now()
                formatted_start_datetime = timestamp_str.strftime('%d/%m/%Y %H:%M:%S')
                bot.send_message(mensaje.chat.id, "Fecha y hora de la petición: <b>" + formatted_start_datetime + "</b>\n\nUtilizando el Cocinero de Inteligencia Artificial en Recetas:\n[<b>" + model + "</b>]\n\n" + "Estoy buscando la mejor respuesta lo antes posible. Me has pedido:\n[<b>" + msg + "</b>]\n", parse_mode="html")
                bot.send_message(mensaje.chat.id, "Dame unos segunditos y enseguida te respondo. Vamos a cocinar tu petición " + emoji_cocinando + " " + emoji_esperando, parse_mode="html")
                response = ollama.chat(model=model, messages=[ # Obtén la respuesta de Ollama usando el modelo Alex y pásale como contenido el mensaje extraído del chat
                    {
                        'role': 'user',
                        'content': msg,
                        'options': {
                            'priority': 'high',  # Prioridad Alta
                            'language': 'es',    # Español
                        },
                    }
                ])
                answer = response['message']['content'] # Extrae la respuesta generada por Ollama
                # html_content = markdown2.markdown(answer)
                answer = answer.replace('**', '')  # Eliminar '**' del texto
                # answer = answer.replace('&quot;', '') 
                # print(answer)
                # answer = html.escape(answer)  # Escapar caracteres especiales
                # respuesta_formateada = f"{answer}\n"
                
                # Timestamp para la fecha y hora de respuesta
                timestamp_str = datetime.now()
                formatted_end_datetime = timestamp_str.strftime('%d/%m/%Y %H:%M:%S')
                bot.send_message(mensaje.chat.id, "Fecha y hora de la respuesta: <b>" + formatted_end_datetime + "</b>\n\n" + answer, parse_mode="html") # Envia la respuesta generada al Chat formateada
    
                # bot.send_message(mensaje.chat.id, "Fecha y hora de la petición: <b>" + formatted_end_datetime + "</b>\n\n" + answer, parse_mode="html") # Envia la respuesta generada al Chat
    
        # Seleccionado el cocinero quiero que cambie al modelo correspondiente
        def gestionar_cambio_cocinero(message):
            new_model = message.text
            if message.text in cocineros:
                data = ollama.list()
    
                # Comprobar si coincide el nombre del modelo (posibles son alex_experto y juan_aprendiz)
                matched = False
                for mod in data['models']:
                    if new_model in mod['name']:
                        matched = True
    
                if not matched: # si no coincide 
                    bot.send_message(message.chat.id, "No he encontrado al Cocinero de Inteligencia Artificial...", parse_mode="html")
                    # ollama.pull(new_model)
                    # bot.send_message(message.chat.id, "Model aplicado con éxito!!!", parse_mode="html")
                
                # Si existe entonces cambiar el modelo
                global model
                model = new_model
                bot.send_message(message.chat.id, "El Cocinero de Inteligencia Artificial seleccionado es <b>[" + new_model + "]</b>", parse_mode="html")
            else:
                bot.send_message(message.chat.id, "El Cocinero de Inteligencia Artificial " + new_model + " no exite o no está soportado!", parse_mode="html")
    
        # Bucle de escucha continuo para recibir y procesar mensajes de Telegram de manera persistente. No se utiliza Webhook.
        # El bot comunica en éste caso utilizando el mecanismo de polling (getUpdates), es decir le pregunta constantemente a Telegram si hay nuevos mensajes
        bot.infinity_polling() 
